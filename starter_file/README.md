# Classification of Wine Quality

In this project, I built two ML models to classify red and white wine quality based on several features. My first model is generated by AutoML and the second one is a pre-selected model whose parameters are hypertuned. Eventually, the results of both models will be compared.

## Dataset

### Overview
We use the following [dataset](https://www.kaggle.com/ruthgn/wine-quality-data-set-red-white-wine?select=wine-quality-white-and-red.csv) provided by Kaggle. This data set contains records related to red and white variants of the Portuguese Vinho Verde wine. It contains information from 1599 red wine samples and 4898 white wine samples. 

### Task
Features in the data set include the type of wine (either red or white wine) and metrics from objective tests (e.g. acidity levels, PH values, ABV, etc.), while the target/output variable (i.e., label) is a numerical score based on sensory dataâ€”median of at least 3 evaluations made by wine experts. Each expert graded the wine quality between 0 (very bad) and 10 (very excellent).

I use the quality as the target label that needs to be classified by the ML engine.

### Access
I uploaded the data in a google [spreadsheet](https://docs.google.com/spreadsheets/d/e/2PACX-1vQ0_ymTF3299kfZvr0KJq5JMLX7ZK4yRg9RXYTqEsMqm2eeUrABv_4MVQMzrqfw1CsbmcrnTqIluMA0/pub?output=csv) and access them via pandas (`read_csv method`).

## Automated ML
In a first step, I applied AutoML provided by AzureML to find the best model for my classification task. There, I used the following settings for the engine:
```
automl_settings = {
       "n_cross_validations": 5,
       "primary_metric": 'AUC_weighted',
       "enable_early_stopping": True,
       "experiment_timeout_hours": 1.0,
       "max_concurrent_iterations": 4,
       "verbosity": logging.INFO}

automl_config = AutoMLConfig(
    compute_target = compute_target,
    task='classification',
    training_data=ds,
    label_column_name='quality',
    **automl_settings)
```
Parameters were chosen based on previous experiments/analysis and default settings.

### Results
As shown in the figure below, the experiment took 25 minutes and completed successfully. 
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_experiment_done.PNG)

AUC (weighted) as the primary metric reached an accuracy of 86.7%. Further metrics are shown below: 
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_result1.PNG)

For the sake of completeness, I also exported some of the main characteristics/results:
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_result2.PNG)
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_confusion_matrix.PNG)

The AutoML engine assessed 37 models and the last one brought an increase to the weighted AUC.
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_model_comparison.PNG)

As shown in the jupyter notebook and also screencast, the most performant model was a Stack Ensemble than completed after only 37 seconds. In the following, I exported the best model. For further information (also on the best estimator), I kindly refer to the notebook output and/or the screencast.
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_bestmodel_studio.PNG)
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_bestmodel.PNG)

Before model selection, AutoML gave an alert in the class balancing detection.
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_alerting.PNG)
To fix the balancing problem and also improve the model's results, I would combine features (e.g., through clustering algorithms) in the next iteration/run to increase class' sizes.

## Hyperparameter Tuning
For that experiment, I pre-selected a logistic regression that is a simple (and easy-to-understand) but also a powerful method when it comes to classification problems. For the hyperparameter tuning, I used the following parameters for sampling and termination:
```
early_termination_policy = BanditPolicy(slack_factor = 0.1, evaluation_interval=2, delay_evaluation=5)

param_sampling = RandomParameterSampling({'--max_iter':20})
```
Parameters were chosen based on previous projects (e.g., Project 1 of the nanodegree) and past experience / default settings.

### Results
The hyperparameter tuning completed successfully after only 4 minutes.
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/hypertune_completed_run.PNG)

As shown below, it only reached an accuracy of 54.77%, which is very low compared to our AutoML model.
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/hypertune_runwidget.PNG)

Best metrics and hyperparameters are here:
```
Run(Experiment: hypertuning-experiment,
Id: HD_1221d562-19e8-481d-b55c-7d9f3abc0a7e,
Type: hyperdrive,
Status: Completed)
run ID:  HD_1221d562-19e8-481d-b55c-7d9f3abc0a7e_0
best metric:  {'Regularization Strength:': 1.0, 'Max iterations:': 20, 'Accuracy': 0.5476923076923077}
hyperparams:  {'runId': 'HD_1221d562-19e8-481d-b55c-7d9f3abc0a7e_0', 'target': 'hypertuning', 'status': 'Completed', 'startTimeUtc': '2021-11-20T13:10:35.024115Z', 'endTimeUtc': '2021-11-20T13:11:16.949423Z', 'services': {}, 'warnings': [{'message': 'This run might be using a new job runtime with improved performance and error reporting. The logs from your script are in user_logs/std_log.txt. Please let us know if you run into any issues, and if you would like to opt-out, please add the environment variable AZUREML_COMPUTE_USE_COMMON_RUNTIME to the environment variables section of the job and set its value to the string "false"'}], 'properties': {'_azureml.ComputeTargetType': 'amlcompute', 'ContentSnapshotId': '78f2f3c8-56a4-48cd-a1b7-f6a53d152589', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'outputDatasets': [], 'runDefinition': {'script': 'train.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--max_iter', '20'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'hypertuning', 'dataReferences': {}, 'data': {}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'Experiment hypertuning-experiment Environment', 'version': 'Autosave_2021-11-20T13:07:45Z_77ed385e', 'python': {'interpreterPath': 'python', 'userManagedDependencies': True, 'condaDependencies': {'name': 'project_environment', 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}], 'channels': ['anaconda', 'conda-forge']}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'sklearn:0.20.3-cpu', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': 'viennaprivate.azurecr.io', 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': False}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'AISupercomputer.D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': [], 'dataBricks': {'workers': 0, 'minimumWorkerCount': 0, 'maxMumWorkerCount': 0, 'sparkVersion': '4.0.x-scala2.11', 'nodeTypeId': 'Standard_D3_v2', 'sparkConf': {}, 'sparkEnvVars': {}, 'instancePoolId': None, 'timeoutSeconds': 0, 'jarLibraries': [], 'eggLibraries': [], 'whlLibraries': [], 'pypiLibraries': [], 'rCranLibraries': [], 'mavenLibraries': [], 'linkedADBWorkspaceMetadata': None, 'databrickResourceId': None, 'autoScale': False}}, 'logFiles': {'logs/azureml/18_azureml.log': 'https://mlstrg164049.blob.core.windows.net/azureml/ExperimentRun/dcid.HD_1221d562-19e8-481d-b55c-7d9f3abc0a7e_0/logs/azureml/18_azureml.log?sv=2019-07-07&sr=b&sig=BLNhP3oLq9bfqQ7%2F3UiUY6pAga%2F%2FlW9llZPBbk0onYo%3D&skoid=a21d3a73-3451-4e87-8066-b9e95a6f2b38&sktid=660b3398-b80e-49d2-bc5b-ac1dc93b5254&skt=2021-11-20T12%3A58%3A07Z&ske=2021-11-21T21%3A08%3A07Z&sks=b&skv=2019-07-07&st=2021-11-20T13%3A05%3A29Z&se=2021-11-20T21%3A15%3A29Z&sp=r'}, 'submittedBy': 'ODL_User 164049'}
```

When comparing the accuracy with the AutoML model, it becomes obvious that the hypertuning still needs some improvement. The poor accuracy indicates that I should not stick with regression (and focus on fine-tuning) in a next iteration but rather try a different model. Since it is a classification problem, my next pre-selection would be a random forest.

## Model Deployment
Afterwards, I deployed the AutoML model that outperformed the other one via ACIWebservice. There, I also enabled app insights so that I could easily generate key to also query the model. 

The following figure shows the successful deployment of the model (in SDK as well as in the endpoint section of the studio).
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/deployment_sdk.PNG)
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/deployment_healthy.PNG)
Both figures also show that the service is healthy and the model can be used.

In a next step, I used to two illustrative data sets to test the model endpoint. The endpoint reacted and gave two predictions back:
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/deployment_endpoint_test.PNG)

I also checked the endpoint test within the studio:
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/deployment_test.PNG)

## Screen Recording
Following the project's structure with two scripts classifying our wine data, I created one [screencast](https://www.youtube.com/watch?v=WEOxwvgcBFQ) to document the AutoML part and an additional [clip](https://www.youtube.com/watch?v=bvrs2v0F9mA) for the hypertuning option. Enjoy watching ;).



