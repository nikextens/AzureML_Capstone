# Classification of Wine Quality

In this project, I built two ML models to classify red and white wine quality based on several features. My first model is generated by AutoML and the second one is a pre-selected model whose parameters are hypertuned. Eventually, the results of both models will be compared.

## Dataset

### Overview
We use the following [dataset](https://www.kaggle.com/ruthgn/wine-quality-data-set-red-white-wine?select=wine-quality-white-and-red.csv) provided by Kaggle. This data set contains records related to red and white variants of the Portuguese Vinho Verde wine. It contains information from 1599 red wine samples and 4898 white wine samples. 

### Task
Features in the data set include the type of wine (either red or white wine) and metrics from objective tests (e.g. acidity levels, PH values, ABV, etc.), while the target/output variable (i.e., label) is a numerical score based on sensory dataâ€”median of at least 3 evaluations made by wine experts. Each expert graded the wine quality between 0 (very bad) and 10 (very excellent).

I use the quality as the target label that needs to be classified by the ML engine.

### Access
I uploaded the data in a google [spreadsheet](https://docs.google.com/spreadsheets/d/e/2PACX-1vQ0_ymTF3299kfZvr0KJq5JMLX7ZK4yRg9RXYTqEsMqm2eeUrABv_4MVQMzrqfw1CsbmcrnTqIluMA0/pub?output=csv) and access them via pandas (`read_csv method`).

## Automated ML
In a first step, I applied AutoML provided by AzureML to find the best model for my classification task. There, I used the following settings for the engine:
```
automl_settings = {
       "n_cross_validations": 5,
       "primary_metric": 'AUC_weighted',
       "enable_early_stopping": True,
       "experiment_timeout_hours": 1.0,
       "max_concurrent_iterations": 4,
       "verbosity": logging.INFO}

automl_config = AutoMLConfig(
    compute_target = compute_target,
    task='classification',
    training_data=ds,
    label_column_name='quality',
    **automl_settings)
```
Parameters were chosen based on previous experiments/analysis and default settings.

### Results
As shown in the figure below, the experiment took 25 minutes and completed successfully. 
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_experiment_done.PNG)

AUC (weighted) as the primary metric reached an accuracy of 86.7%. Further metrics are shown below: 
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_result1.PNG)

For the sake of completeness, I also exported some of the main characteristics/results:
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_result2.PNG)
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_confusion_matrix.PNG)

The AutoML engine assessed 37 models and the last one brought an increase to the weighted AUC.
![plot](https://github.com/nikextens/AzureML_Capstone/blob/master/starter_file/Screenshots/automl_model_comparison.PNG)

As shown in the jupyter notebook and also screencast, the most performant model was a Stack Ensemble than completed after only 37 seconds. In the following, I exported the best model. For further information (also on the best estimator), I kindly refer to the notebook output and/or the screencast.
```
Best model:  Pipeline(memory=None,
         steps=[('datatransformer',
                 DataTransformer(enable_dnn=False, enable_feature_sweeping=True, feature_sweeping_config={}, feature_sweeping_timeout=86400, featurization_config=None, force_text_dnn=False, is_cross_validation=True, is_onnx_compatible=False, observer=None, task='classification', working_dir='/mnt/batch/tasks/shared/LS_root/mount...
)), ('logisticregression', LogisticRegression(C=2222.996482526191, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='multinomial', n_jobs=1, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False))], verbose=False))], meta_learner=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), training_cv_folds=5))],
         verbose=False)
```

*TODO*: What are the results you got with your automated ML model? What were the parameters of the model? How could you have improved it?

*TODO* Remeber to provide screenshots of the `RunDetails` widget as well as a screenshot of the best model trained with it's parameters.

## Hyperparameter Tuning
*TODO*: What kind of model did you choose for this experiment and why? Give an overview of the types of parameters and their ranges used for the hyperparameter search


### Results
*TODO*: What are the results you got with your model? What were the parameters of the model? How could you have improved it?

*TODO* Remeber to provide screenshots of the `RunDetails` widget as well as a screenshot of the best model trained with it's parameters.

## Model Deployment
*TODO*: Give an overview of the deployed model and instructions on how to query the endpoint with a sample input.

## Screen Recording
*TODO* Provide a link to a screen recording of the project in action. Remember that the screencast should demonstrate:
- A working model
- Demo of the deployed  model
- Demo of a sample request sent to the endpoint and its response

## Standout Suggestions
*TODO (Optional):* This is where you can provide information about any standout suggestions that you have attempted.
